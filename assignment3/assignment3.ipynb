{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random as r\n",
    "import sympy\n",
    "import queue as q\n",
    "import numpy as np\n",
    "import array\n",
    "from pandas import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing(obbject, qq, ww, mmod):\n",
    "    obbbject.q= qq\n",
    "    obbject.w = ww\n",
    "    obbject.mod= modd\n",
    "    \n",
    "def hashingCompute(obbject, e):\n",
    "    return(obbject.q*e + obbject.w)% obbject.mod\n",
    "\n",
    "def adding(obbject)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minSample(object):\n",
    "\n",
    "    def __init__(a, maximumSize):\n",
    "        a.ms = maximumSize\n",
    "        a.sample = q.PriorityQueue(maxsize=maximumSize)\n",
    "\n",
    "    def adding(a, j): # This method used values between 0 - 100 which are uniform. It will check if the new tuple contains a higher score than the first element in the priorityqueue. It removes this first tuple when the score is lower and then will adding a new.\n",
    "            # And the else statement if the tuple which is orginal is putted back , or when the priority is not full.\n",
    "        \n",
    "        score = r.uniform(0, 100)\n",
    "        tuple = (score, j)\n",
    "        if a.sample.full():\n",
    "            firstElement = a.sample.get()\n",
    "            firstElementScore = firstElement[0]\n",
    "            if score > firstElementScore:\n",
    "                a.sample.put(tuple) \n",
    "            else:\n",
    "                a.sample.put(firstElement) \n",
    "     \n",
    "        else:\n",
    "            a.sample.put(tuple)\n",
    "\n",
    "    def countSort(a):# This method is sorting in a descending order with the use of a Priority Queue\n",
    "        frequency = a.counting()\n",
    "        pqueue = q.PriorityQueue()\n",
    "        \n",
    "        for ip in frequency:\n",
    "            \n",
    "            pqueue.put((a.ms - frequency[ip], ip))\n",
    "            \n",
    "        result = dict()\n",
    "        \n",
    "        while not pqueue.empty():\n",
    "            j = pqueue.get()\n",
    "            result[j[1]] = a.ms - j[0]\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def counting(a): # This method is counting the frequencies of the IP addresses which are sampled\n",
    "        counting = dict()\n",
    "\n",
    "        while not a.sample.empty():\n",
    "            ip = a.sample.get()[1]\n",
    "            if ip in counting:\n",
    "                counting[ip] = counting[ip]+ 1\n",
    "            else:\n",
    "                counting[ip] = 1\n",
    "        return counting\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def sortValue(frequencies, maximumValue): #This method stores the frequencies in a descending order\n",
    "        pqueue2 = q.PriorityQueue()\n",
    "        for ip in frequencies:\n",
    " \n",
    "            pqueue2.put((maximumValue - frequencies[ip], ip))\n",
    "        result = dict()\n",
    "        \n",
    "        while not pqueue2.empty():\n",
    "            j = pqueue2.get() \n",
    "            result[j[1]] = maximumValue - j[0]\n",
    "        return result\n",
    "\n",
    "def removeEmpty(a): # Remove the empty strings\n",
    "    result = []\n",
    "    \n",
    "    for d in a:\n",
    "        if not d == '':\n",
    "            result.append(d)\n",
    "    return result  \n",
    "    \n",
    "def first_k(dictionary, k):\n",
    "    result = dict()\n",
    "    keyset = list(dictionary.keys())\n",
    "    for i in range(k):\n",
    "        key = keyset[i]\n",
    "        result[key] = dictionary[key]\n",
    "    return result\n",
    "\n",
    "def samePosition(d1, d2): # Keys in same position\n",
    "    counting = 0\n",
    "    key1, key2 = list(d1.keys()), list(d2.keys())\n",
    "    for i in range(len(key1)):\n",
    "        if key1[i] == key2[i]:\n",
    "            counting = counting + 1\n",
    "    return counting\n",
    "    \n",
    "def common(d1, d2): # Keys in common\n",
    "    counting = 0\n",
    "    for k in d1:\n",
    "        if k in d2:\n",
    "            counting = counting + 1\n",
    "    return counting    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class packets(object):\n",
    "\n",
    "    def __init__(a, start_, dura_, protocol_, src_, dst_, flags_, tos_, packets_, byte_, flows_, label_):\n",
    "        a.dateformat = '%Y-%m-%d %H:%M:%S.%f'\n",
    "        a.start = datetime.strptime(start_, a.dateformat)\n",
    "        a.flags = flags_\n",
    "        a.src = src_\n",
    "        a.duration = float(dura_) \n",
    "        a.dst = dst_\n",
    "        a.protocol = protocol_\n",
    "        a.label = label_\n",
    "        a.tos = int(tos_)\n",
    "        a.packets = int(packets_) \n",
    "        a.bytes = int(byte_)\n",
    "        a.flows = int(flows_)\n",
    "        \n",
    "\n",
    "    def __str__(a):\n",
    "        result = '[Packet, start_time = %s, duration = %f, protocol = %s, source = %s, destination = %s, flags = %s, TOS = %i, packets = %i, ' \\\n",
    "              'bytes = %i, flows = %i, label = %s]' % (datetime.strftime(a.start, a.dateformat), a.duration, a.protocol, a.src, a.dst, a.flags,\n",
    "                                                       a.tos, a.packets, a.bytes, a.flows, a.label)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###SAMPLING TASK### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10 highest frequencies:\n",
      "{'147.32.84.229': 60842, '147.32.84.59': 49223, '147.32.80.9': 47001, '147.32.84.138': 19607, '147.32.84.94': 15489, '147.32.80.13': 5817, '147.32.85.7': 4235, '147.32.85.25': 3242, '147.32.85.8': 3152, '147.32.86.134': 2516}\n",
      " 10 highest sampled frequencies with a reservoir size of 100:\n",
      "{'147.32.84.229': 137, '147.32.80.9': 103, '147.32.84.59': 99, '147.32.84.94': 42, '147.32.84.138': 33, '147.32.80.13': 20, '147.32.86.134': 9, '147.32.85.7': 7, '147.32.85.25': 6, '147.32.85.34': 6}\n",
      "\n",
      "\n",
      "9 IP addresses are in common for the last two sets, and 2 having the same position\n",
      " 10 highest sampled frequencies with a reservoir size of 1000:\n",
      "{'147.32.84.229': 1349, '147.32.84.59': 1066, '147.32.80.9': 1017, '147.32.84.138': 432, '147.32.84.94': 337, '147.32.80.13': 137, '147.32.85.7': 97, '147.32.85.8': 84, '147.32.85.25': 74, '147.32.86.187': 54}\n",
      "\n",
      "\n",
      "9 IP addresses are in common for the last two sets, and 7 having the same position\n",
      " 10 highest sampled frequencies with a reservoir size of 10000:\n",
      "{'147.32.84.229': 13471, '147.32.84.59': 10898, '147.32.80.9': 10365, '147.32.84.138': 4295, '147.32.84.94': 3478, '147.32.80.13': 1308, '147.32.85.7': 931, '147.32.85.25': 754, '147.32.85.8': 692, '147.32.86.134': 576}\n",
      "\n",
      "\n",
      "10 IP addresses are in common for the last two sets, and 10 having the same position\n"
     ]
    }
   ],
   "source": [
    "frequency = dict()\n",
    "amount = 0\n",
    "k= 10\n",
    "size = [1000,10000, 100000]\n",
    "adressList = [] \n",
    "#samples = minSample(size)\n",
    "samples = [] \n",
    "\n",
    "for g in size:\n",
    "    samples.append(minSample(g))\n",
    "\n",
    "\n",
    "with open(\"Data\\capture20110816-2.pcap.netflow.labeled\", \"r\") as data:\n",
    "    reader = csv.reader(data, delimiter=\" \")\n",
    "    for a, line in enumerate(reader):\n",
    "        if a < 1:\n",
    "            continue\n",
    "        \n",
    "        splitting = line[1].split(\"\\t\")\n",
    "        newSplitting = removeEmpty(splitting)\n",
    "        dateTime = line[0] + ' ' + newSplitting[0]\n",
    "        packet = packets(dateTime, newSplitting[1], newSplitting[2], newSplitting[3], newSplitting[5], newSplitting[6], newSplitting[7], newSplitting[8], newSplitting[9], newSplitting[10], newSplitting[11])\n",
    "        ip = packet.dst.split(\":\")[0]\n",
    "        \n",
    "                \n",
    "        if(ip!= \"ff02\") and (ip!= \"Broadcast\"):\n",
    "                    \n",
    "                    for s in samples: \n",
    "                        s.adding(ip) \n",
    "                    if ip in frequency:\n",
    "                        frequency[ip] = frequency[ip]+ 1 \n",
    "                        \n",
    "                    else:\n",
    "                        frequency[ip] = 1\n",
    "                    \n",
    "                    adressList.append(ip) \n",
    "                    amount = amount+ 1\n",
    "                    \n",
    "                                       \n",
    "#print(ip) \n",
    "          \n",
    "totalSorting = sortValue(frequency,amount) \n",
    "\n",
    "for t, s in enumerate(samples):\n",
    "    samples[t]= s.countSort()\n",
    "    \n",
    "\n",
    "#print('All frequencies in a descending order:') \n",
    "#print(totalSorting) \n",
    "#print('Samples frequencies with a size of 10000, which is sorted in descending order') \n",
    "#print(samplingSorting)\n",
    "\n",
    "allFirst_k = first_k(totalSorting,k)\n",
    "\n",
    "print(' %i highest frequencies:' % k ) \n",
    "print(allFirst_k)  \n",
    "\n",
    "\n",
    "\n",
    "for y, s in enumerate(samples):\n",
    "    sampledFirst_k = first_k(s, k) \n",
    "    print(' {} highest sampled frequencies with a reservoir size of {}:'.format(k, 10**(y+2)))#':' % k ) \n",
    "    print(sampledFirst_k) \n",
    "    print('\\n') \n",
    "    print('%i IP addresses are in common for the last two sets, and %i having the same position' % (common(allFirst_k, sampledFirst_k), samePosition(allFirst_k, sampledFirst_k) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###SKETCHING TASK###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minSketch(object): # This method contains a constant look up time and insertion and uses the same amount of memory\n",
    "    # Parameter m is the amount of hashtables (When larger: smaller overestimation probability) and s is the size of the hashtables. (When larger: smaller overestimation)\n",
    "   \n",
    "\n",
    "    def __init__(a, s, m): # m->s  d->m\n",
    "      \n",
    "        if not s or not m:\n",
    "            raise ValueError(\"Amount and size must be not zero\")\n",
    "        a.s = s\n",
    "        a.tables = []\n",
    "        a.n = 0\n",
    "        a.m = m\n",
    "        a.salt = []\n",
    "        for _ in range(m):\n",
    "            table = array.array(\"l\", (0 for _ in range(s)))\n",
    "            a.tables.append(table)\n",
    "            a.salt.append(hash(str(id(table))))\n",
    "            \n",
    "    def adding(a, b, v=1): # This method counts the element b when appeared v times. \n",
    "        a.n = a.n+ v\n",
    "        xx = []\n",
    "        for table, u in zip(a.tables, a._hash(b)):\n",
    "            tables[u] = tables[u] + value\n",
    "            xx.append(tables[u])\n",
    "        return min(xx)  # TODO: Add test        \n",
    "            \n",
    "\n",
    "    def _hashing(a, w): #hash\n",
    "        for j in a.salt:\n",
    "            # print(hash((w, j)))\n",
    "            yield hashing((w, j)) % a.s\n",
    "\n",
    "    def query(a, i): # In this method , the returnment of i, is an overestimated value of the real value\n",
    "        \n",
    "        return min(table[u] for table, u in zip(a.tables, a._hash(i)))\n",
    "\n",
    "\n",
    "    def __count__(a): # counting amount #len\n",
    "       \n",
    "        return a.n\n",
    "    \n",
    "    \n",
    "    def __getItem__(a, z):\n",
    "        \n",
    "        return a.query(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nog bewerken alles\n",
    "\n",
    "sizes = [10, 100, 1000, 10000] # Sizes of the tables\n",
    "num_hash_functions = [10, 20, 30]\n",
    "\n",
    "file = \"data\\capture20110816-2.pcap.netflow.labeled\"\n",
    "ip_set = set()\n",
    "ip_amt = 0\n",
    "k = 10\n",
    "\n",
    "sketches = np.empty(((len(table_sizes), len(num_hash_functions))), dtype=object)\n",
    "\n",
    "print(\"Initializing min sketches\")\n",
    "for x in range(len(table_sizes)):\n",
    "    for y in range(len(num_hash_functions)):\n",
    "        sketches[x][y] = cms.CountMinSketch(table_sizes[x], num_hash_functions[y])\n",
    "\n",
    "# Treat data as a stream.\n",
    "with open(file, \"r\") as f:\n",
    "    print(\"Reading data\")\n",
    "    reader = csv.reader(f, delimiter=\" \")\n",
    "    for z, line in enumerate(reader):\n",
    "        if z < 1:\n",
    "            continue\n",
    "\n",
    "        # Split the arguments in the line\n",
    "        args = line[1].split(\"\\t\")\n",
    "        new_args = remove_empty_strings(args)\n",
    "        date = line[0] + ' ' + new_args[0]\n",
    "        p = packet(date, new_args[1], new_args[2], new_args[3], new_args[5], new_args[6], new_args[7], new_args[8],\n",
    "                    new_args[9], new_args[10], new_args[11])\n",
    "\n",
    "        ip = p.dst.split(\":\")[0]\n",
    "\n",
    "        # Filter the broadcasts and non-ip adresses\n",
    "        if (ip != \"Broadcast\") and (ip != \"ff02\"):\n",
    "            for x in range(len(table_sizes)):\n",
    "                for y in range(len(num_hash_functions)):\n",
    "                    sketches[x][y].add(ip)\n",
    "            ip_set.add(ip)\n",
    "            ip_amt += 1\n",
    "\n",
    "print('Stream reading done.')\n",
    "print('The file that was read can be found in %s' % file)\n",
    "print('There are %i destination IP addresses' % ip_amt)\n",
    "\n",
    "for x in range(len(table_sizes)):\n",
    "    for y in range(len(num_hash_functions)):\n",
    "        ip_dict = dict()\n",
    "        for ip in ip_set:\n",
    "            ip_dict[ip] = sketches[x][y][ip]\n",
    "\n",
    "        ip_dict = sort_dict_by_value(ip_dict, ip_amt)\n",
    "        # print('Total sketched frequencies, generated with table_size=%i and hash_functions=%i, sorted by value in descending order:' % (table_size, hash_functions))\n",
    "        # print(ip_dict)\n",
    "        top_k = select_first_k(ip_dict, k)\n",
    "        print('Top {} highest sketched frequencies with column length {} and {} hash functions:'.format(k, table_sizes[x], num_hash_functions[y]))\n",
    "print(top_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
